{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9_AidRNPA9MP"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYAMfaVy3N4vU+rFkkn8a5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tunatone0111/2020-summer-research/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd12YNiX_cA1",
        "colab_type": "text"
      },
      "source": [
        "# Project\n",
        "Only runs for python3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_AidRNPA9MP",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho4BJP4X-X4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "fb9c833a-1d3c-4bbb-909f-d0faf7488e19"
      },
      "source": [
        "!pip install tensorflow-gpu==2.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.30.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.9.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qf965U_M2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "81e7bb8d-13e0-458e-94fc-99547c15b0ab"
      },
      "source": [
        "!pip install tensorflow_probability==0.11.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_probability==0.11.0 in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.0) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.0) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.0) (1.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.0) (0.1.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.0) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability==0.11.0) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYcISwMp_PZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a21a29d0-23ee-46cf-9fcf-e835a1131256"
      },
      "source": [
        "!pip install attr"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting attr\n",
            "  Downloading https://files.pythonhosted.org/packages/de/be/ddc7f84d4e087144472a38a373d3e319f51a6faf6e5fc1ae897173675f21/attr-0.3.1.tar.gz\n",
            "Building wheels for collected packages: attr\n",
            "  Building wheel for attr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for attr: filename=attr-0.3.1-cp36-none-any.whl size=2459 sha256=1e245e114d2815081a323261f8ccf9890c1a82b2f0f5d0cd83c396dff89ce7ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/96/9b/1f8892a707d17095b5a6eab0275da9d39e68e03a26aee2e726\n",
            "Successfully built attr\n",
            "Installing collected packages: attr\n",
            "Successfully installed attr-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68sEF-24_w8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import attr\n",
        "import functools\n",
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpagR-I7BKXX",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzYKIXJ4B4zD",
        "colab_type": "text"
      },
      "source": [
        "### for variational layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBwr8tQGB-e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfd = tfp.distributions\n",
        "\n",
        "\n",
        "def tfp_layer_with_scaled_kl(layer_builder, num_train_examples):\n",
        "    def scaled_kl_fn(q, p, _):\n",
        "        return tfd.kl_divergence(q, p) / num_train_examples\n",
        "\n",
        "    return functools.partial(layer_builder, kernel_divergence_fn=scaled_kl_fn)\n",
        "\n",
        "\n",
        "def _posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
        "    \"\"\"Posterior function for variational layer.\"\"\"\n",
        "    n = kernel_size + bias_size\n",
        "    c = np.log(np.expm1(1e-5))\n",
        "    variable_layer = tfp.layers.VariableLayer(\n",
        "        2 * n, dtype=dtype,\n",
        "        initializer=tfp.layers.BlockwiseInitializer([\n",
        "            keras.initializers.TruncatedNormal(mean=0., stddev=.05, seed=None),\n",
        "            keras.initializers.Constant(np.log(np.expm1(1e-5)))], sizes=[n, n]))\n",
        "\n",
        "    def distribution_fn(t):\n",
        "        scale = 1e-5 + tf.nn.softplus(c + t[Ellipsis, n:])\n",
        "        return tfd.Independent(tfd.Normal(loc=t[Ellipsis, :n], scale=scale),\n",
        "                               reinterpreted_batch_ndims=1)\n",
        "    distribution_layer = tfp.layers.DistributionLambda(distribution_fn)\n",
        "    return tf.keras.Sequential([variable_layer, distribution_layer])\n",
        "\n",
        "\n",
        "def _make_prior_fn(kernel_size, bias_size=0, dtype=None):\n",
        "    del dtype  # TODO(yovadia): Figure out what to do with this.\n",
        "    loc = tf.zeros(kernel_size + bias_size)\n",
        "\n",
        "    def distribution_fn(_):\n",
        "        return tfd.Independent(tfd.Normal(loc=loc, scale=1),\n",
        "                               reinterpreted_batch_ndims=1)\n",
        "    return distribution_fn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpyQXhgkJiz_",
        "colab_type": "text"
      },
      "source": [
        "### layer builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08HQp-c9BM_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_layer_builders(method, num_train_examples):\n",
        "  \"\"\"Get method-appropriate functions for building and/or applying Keras layers.\n",
        "\n",
        "  Args:\n",
        "    method: UQ method (vanilla, svi).\n",
        "    num_train_examples: Number of training examples. Used to scale KL loss.\n",
        "  Returns:\n",
        "    conv2d, dense_layer\n",
        "  \"\"\"\n",
        "    if method == 'vanilla':\n",
        "        return keras.layers.Conv2D, keras.layers.Dense\n",
        "    \n",
        "    tfpl = tfp.layers\n",
        "\n",
        "    conv2d_variational = tfp_layer_with_scaled_kl(tfpl.Convolution2DFlipout,\n",
        "                                                  num_train_examples)\n",
        "    # Only DenseVariational works in v2 / eager mode.\n",
        "    # FMI: https://github.com/tensorflow/probability/issues/409\n",
        "    if tf.executing_eagerly():\n",
        "        def dense_variational(units, activation):\n",
        "            return tfpl.DenseVariational(\n",
        "                units,\n",
        "                make_posterior_fn=_posterior_mean_field,\n",
        "                make_prior_fn=_make_prior_fn,\n",
        "                activation=activation,\n",
        "                kl_weight=1./num_train_examples)\n",
        "    else:\n",
        "        dense_variational = tfp_layer_with_scaled_kl(tfpl.DenseFlipout,\n",
        "                                                     num_train_examples)\n",
        "\n",
        "    return conv2d_variational, dense_variational"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhc09AtPDI2P",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cUyQt2mDK8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelOptions(object):\n",
        "    \"\"\"Parameters for model construction and fitting.\"\"\"\n",
        "    train_epochs = attr.ib()\n",
        "    num_train_examples = attr.ib()\n",
        "    batch_size = attr.ib()\n",
        "    learning_rate = attr.ib()\n",
        "    method = attr.ib()\n",
        "    num_examples_for_predict = attr.ib()\n",
        "    predictions_per_example = attr.ib()\n",
        "    input_shape = attr.ib()\n",
        "    num_classes = attr.ib()\n",
        "\n",
        "\n",
        "def build_model(opts):\n",
        "  \"\"\"Builds a VGGNet Keras model.\"\"\"\n",
        "  layer_builders = get_layer_builders(opts.method, opts.num_train_examples)\n",
        "  conv2d, dense_layer = layer_builders\n",
        "\n",
        "  inputs = keras.layers.Input(opts.input_shape)\n",
        "  net = inputs\n",
        "  logits = dense_last(opts.num_classes)(net)\n",
        "  return keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "\n",
        "def build_and_train(opts, dataset_train, dataset_eval, output_dir):\n",
        "  \"\"\"Returns a trained MNIST model and saves it to output_dir.\n",
        "\n",
        "  Args:\n",
        "    opts: ModelOptions\n",
        "    dataset_train: Pair of images, labels np.ndarrays for training.\n",
        "    dataset_eval: Pair of images, labels np.ndarrays for continuous eval.\n",
        "    output_dir: Directory for the saved model and tensorboard events.\n",
        "  Returns:\n",
        "    Trained Keras model.\n",
        "  \"\"\"\n",
        "  model = build_model(opts)\n",
        "  model.compile(\n",
        "      keras.optimizers.Adam(opts.learning_rate),\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['accuracy'],\n",
        "  )\n",
        "\n",
        "  tensorboard_cb = keras.callbacks.TensorBoard(\n",
        "      log_dir=output_dir, write_graph=False)\n",
        "\n",
        "  train_images, train_labels = dataset_train\n",
        "  assert len(train_images) == opts.num_train_examples, (\n",
        "      '%d != %d' % (len(train_images), opts.num_train_examples))\n",
        "  model.fit(\n",
        "      train_images, train_labels,\n",
        "      epochs=opts.train_epochs,\n",
        "      # NOTE: steps_per_epoch will cause OOM for some reason.\n",
        "      validation_data=dataset_eval,\n",
        "      batch_size=opts.batch_size,\n",
        "      callbacks=[tensorboard_cb],\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoqY5LjrAwdZ",
        "colab_type": "text"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8WgVgxwMrfZ",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd7y-b9_MqtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(opts):\n",
        "  \"\"\"Returns an <images, labels> dataset pair.\"\"\"\n",
        "  opts = MnistDataOptions(**opts)\n",
        "  logging.info('Building dataset with options: %s', opts)\n",
        "\n",
        "  # We can't use in-distribution data from tfds due to inconsistent orderings.\n",
        "  if opts.dataset_name == 'mnist':\n",
        "    dataset = _mnist_dataset_from_tfr(opts.split)\n",
        "  elif opts.dataset_name == 'not_mnist':\n",
        "    dataset = _not_mnist_dataset_from_tfr(opts.split)\n",
        "  else:\n",
        "    dataset = _dataset_from_tfds(opts.dataset_name, opts.split)\n",
        "\n",
        "  # Download dataset to memory.\n",
        "  images, labels = list(zip(*tfds.as_numpy(dataset.batch(10**4))))\n",
        "  images = np.concatenate(images, axis=0).astype(np.float32)\n",
        "  labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "  images /= 255\n",
        "  if opts.rotate_degs:\n",
        "    images = scipy.ndimage.rotate(images, opts.rotate_degs, axes=[-2, -3])\n",
        "    images = _crop_center(images, 28)\n",
        "  if opts.roll_pixels:\n",
        "    images = np.roll(images, opts.roll_pixels, axis=-2)\n",
        "\n",
        "  return images, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zczdMErAosF",
        "colab_type": "text"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE5lKefLAtxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_BATCH_SIZE_FOR_PREDICT = 100\n",
        "\n",
        "HParams = collections.namedtuple(\n",
        "    'Hparams', ['batch_size', 'learning_rate']\n",
        ")\n",
        "\n",
        "\n",
        "_HPS_DICT = {\n",
        "    'mnist'=dict(vanilla=HParams(64, 0.0004), svi=HParams(2048, 0.003)),\n",
        "    'cifar'=dict(vanilla=HParams(64, 0.0004), svi=HParams(2048, 0.003))}\n",
        "\n",
        "\n",
        "def get_tuned_model_options(method):\n",
        "  hps = _HPS_DICT[method]\n",
        "\n",
        "  num_train_examples = (data.lib.NUM_TRAIN_EXAMPLES)\n",
        "  model_opts = models_lib.ModelOptions(\n",
        "      method=method,\n",
        "      train_epochs=_TRAIN_EPOCHS,\n",
        "      num_train_examples=num_train_examples,\n",
        "      batch_size=hps.batch_size,\n",
        "      learning_rate=hps.learning_rate,\n",
        "      num_examples_for_predict=int(1e4),\n",
        "      predictions_per_example=_PREDICTIONS_PER_EXAMPLE,\n",
        "      input_shape=(28, 28),\n",
        "      num_classes=10\n",
        "  )\n",
        "\n",
        "  if method == 'vanilla':\n",
        "    model_opts.predictions_per_example = 1\n",
        "  return model_opts"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saOPwil4KIRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gfile = tf.io.gfile\n",
        "\n",
        "def get_experiment_config(method,\n",
        "                          test_level, output_dir=None):\n",
        "  \"\"\"Returns model and data configs.\"\"\"\n",
        "  data_opts_list = data_lib.DATA_OPTIONS_LIST\n",
        "  if test_level:\n",
        "    data_opts_list = data_opts_list[:4]\n",
        "\n",
        "  model_opts = hparams_lib.get_tuned_model_options(method)\n",
        "\n",
        "  if output_dir:\n",
        "    experiment_utils.record_config(model_opts, output_dir+'/model_options.json')\n",
        "  return model_opts, data_opts_list\n",
        "\n",
        "\n",
        "def run(method, output_dir, test_level):\n",
        "  \"\"\"Trains a model and records its predictions on configured datasets.\n",
        "\n",
        "  Args:\n",
        "    method: Name of modeling method (vanilla, svi).\n",
        "    output_dir: Directory to record the trained model and output stats.\n",
        "    test_level: Zero indicates no testing. One indicates testing with real data.\n",
        "  \"\"\"\n",
        "  gfile.makedirs(output_dir)\n",
        "  model_opts, data_opts_list = get_experiment_config(method,\n",
        "                                                     test_level=test_level,\n",
        "                                                     output_dir=output_dir)\n",
        "\n",
        "  # Separately build dataset[0] with shuffle=True for training.\n",
        "  dataset_train = data_lib.build_dataset(data_opts_list[0])\n",
        "  dataset_eval = data_lib.build_dataset(data_opts_list[1])\n",
        "  model = models_lib.build_and_train(model_opts,\n",
        "                                     dataset_train, dataset_eval, output_dir)\n",
        "  logging.info('Saving model to output_dir.')\n",
        "  model.save_weights(output_dir + '/model.ckpt')\n",
        "\n",
        "  for idx, data_opts in enumerate(data_opts_list):\n",
        "    dataset = data_lib.build_dataset(data_opts)\n",
        "    logging.info('Running predictions for dataset #%d', idx)\n",
        "    stats = models_lib.make_predictions(model_opts, model, dataset)\n",
        "    array_utils.write_npz(output_dir, 'stats_%d.npz' % idx, stats)\n",
        "    del stats['logits_samples']\n",
        "    array_utils.write_npz(output_dir, 'stats_small_%d.npz' % idx, stats)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzI5kGJEAzdm",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR-10"
      ]
    }
  ]
}