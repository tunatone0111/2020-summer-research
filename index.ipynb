{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "keras = tf.keras\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def tfp_layer_with_scaled_kl(layer_builder, num_train_examples):\n",
    "    def scaled_kl_fn(q, p, _):\n",
    "        return tfd.kl_divergence(q, p) / num_train_examples\n",
    "\n",
    "    return functools.partial(layer_builder, kernel_divergence_fn=scaled_kl_fn)\n",
    "\n",
    "\n",
    "def get_layer_builders(method, num_train_examples):\n",
    "    \"\"\"Get method-appropriate functions for building and/or applying Keras layers.\n",
    "    \n",
    "    Args:\n",
    "        method: UQ method (vanilla, svi).\n",
    "        num_train_examples: Number of training examples. Used to scale KL loss.    \n",
    "    Returns:\n",
    "        conv2d, dense_layer\n",
    "    \"\"\"\n",
    "    tfpl = tfp.layers\n",
    "\n",
    "    conv2d_variational = tfp_layer_with_scaled_kl(tfpl.Convolution2DFlipout,\n",
    "                                                num_train_examples)\n",
    "  # Only DenseVariational works in v2 / eager mode.\n",
    "  # FMI: https://github.com/tensorflow/probability/issues/409\n",
    "    if tf.executing_eagerly():\n",
    "        def dense_variational(units, activation):\n",
    "            return tfpl.DenseVariational(\n",
    "                units,\n",
    "                make_posterior_fn=_posterior_mean_field,\n",
    "                make_prior_fn=_make_prior_fn,\n",
    "                activation=activation,\n",
    "                kl_weight=1./num_train_examples)\n",
    "    else:\n",
    "        dense_variational = tfp_layer_with_scaled_kl(tfpl.DenseFlipout,\n",
    "                                                     num_train_examples)\n",
    "    \n",
    "    if method == 'svi':\n",
    "        return conv2d_variational, dense_variational\n",
    "    else\n",
    "        return keras.layers.Conv2D, keras.layers.Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptions(object):\n",
    "  \"\"\"Parameters for model construction and fitting.\"\"\"\n",
    "  train_epochs = attr.ib()\n",
    "  num_train_examples = attr.ib()\n",
    "  batch_size = attr.ib()\n",
    "  learning_rate = attr.ib()\n",
    "  method = attr.ib()\n",
    "  architecture = attr.ib()\n",
    "  mlp_layer_sizes = attr.ib()\n",
    "  num_examples_for_predict = attr.ib()\n",
    "  predictions_per_example = attr.ib()\n",
    "\n",
    "def _build_lenet(opts):\n",
    "  \"\"\"Builds a LeNet Keras model.\"\"\"\n",
    "  layer_builders = uq_utils.get_layer_builders(opts.method,\n",
    "                                               opts.num_train_examples)\n",
    "  conv2d, dense_layer = layer_builders\n",
    "\n",
    "  inputs = keras.layers.Input(_MNIST_SHAPE)\n",
    "  net = inputs\n",
    "  net = conv2d(32, kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               input_shape=_MNIST_SHAPE)(net)\n",
    "  net = conv2d(64, (3, 3), activation='relu')(net)\n",
    "  net = keras.layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "  net = keras.layers.Flatten()(net)\n",
    "  net = dense_layer(128, activation='relu')(net)\n",
    "  logits = dense_layer(_NUM_CLASSES)(net)\n",
    "\n",
    "  return keras.Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(opts):\n",
    "  \"\"\"Builds (uncompiled) Keras model from ModelOptions instance.\"\"\"\n",
    "  return {'mlp': _build_mlp, 'lenet': _build_lenet}[opts.architecture](opts)\n",
    "\n",
    "def build_and_train(opts, dataset_train, dataset_eval, output_dir):\n",
    "  \"\"\"Returns a trained MNIST model and saves it to output_dir.\n",
    "\n",
    "  Args:\n",
    "    opts: ModelOptions\n",
    "    dataset_train: Pair of images, labels np.ndarrays for training.\n",
    "    dataset_eval: Pair of images, labels np.ndarrays for continuous eval.\n",
    "    output_dir: Directory for the saved model and tensorboard events.\n",
    "  Returns:\n",
    "    Trained Keras model.\n",
    "  \"\"\"\n",
    "  model = build_model(opts)\n",
    "  model.compile(\n",
    "      keras.optimizers.Adam(opts.learning_rate),\n",
    "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "      log_dir=output_dir, write_graph=False)\n",
    "\n",
    "  train_images, train_labels = dataset_train\n",
    "  assert len(train_images) == opts.num_train_examples, (\n",
    "      '%d != %d' % (len(train_images), opts.num_train_examples))\n",
    "  model.fit(\n",
    "      train_images, train_labels,\n",
    "      epochs=opts.train_epochs,\n",
    "      # NOTE: steps_per_epoch will cause OOM for some reason.\n",
    "      validation_data=dataset_eval,\n",
    "      batch_size=opts.batch_size,\n",
    "      callbacks=[tensorboard_cb],\n",
    "  )\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(opts, model, dataset):\n",
    "  \"\"\"Build a dictionary of model predictions on a given dataset.\n",
    "\n",
    "  Args:\n",
    "    opts: ModelOptions.\n",
    "    model: Trained Keras model.\n",
    "    dataset: tf.data.Dataset of <image, label> pairs.\n",
    "  Returns:\n",
    "    Dictionary containing labels and model logits.\n",
    "  \"\"\"\n",
    "  if opts.num_examples_for_predict:\n",
    "    dataset = tuple(x[:opts.num_examples_for_predict] for x in dataset)\n",
    "\n",
    "  batched_dataset = (tf.data.Dataset.from_tensor_slices(dataset)\n",
    "                     .batch(_BATCH_SIZE_FOR_PREDICT))\n",
    "  out = collections.defaultdict(list)\n",
    "  for images, labels in tfds.as_numpy(batched_dataset):\n",
    "    logits_samples = np.stack(\n",
    "        [model.predict(images) for _ in range(opts.predictions_per_example)],\n",
    "        axis=1)  # shape: [batch_size, num_samples, num_classes]\n",
    "    probs = scipy.special.softmax(logits_samples, axis=-1).mean(-2)\n",
    "    out['labels'].extend(labels)\n",
    "    out['logits_samples'].extend(logits_samples)\n",
    "    out['probs'].extend(probs)\n",
    "    if len(out['image_examples']) < _NUM_IMAGE_EXAMPLES_TO_RECORD:\n",
    "      out['image_examples'].extend(images)\n",
    "\n",
    "  return {k: np.stack(a) for k, a in six.iteritems(out)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_config(method, architecture,\n",
    "                          test_level, output_dir=None):\n",
    "  \"\"\"Returns model and data configs.\"\"\"\n",
    "  data_opts_list = data_lib.DATA_OPTIONS_LIST\n",
    "  if test_level:\n",
    "    data_opts_list = data_opts_list[:4]\n",
    "\n",
    "  model_opts = hparams_lib.get_tuned_model_options(architecture, method,\n",
    "                                                   fake_data=test_level > 1,\n",
    "                                                   fake_training=test_level > 0)\n",
    "  if output_dir:\n",
    "    experiment_utils.record_config(model_opts, output_dir+'/model_options.json')\n",
    "  return model_opts, data_opts_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(method, architecture, output_dir, test_level):\n",
    "  \"\"\"Trains a model and records its predictions on configured datasets.\n",
    "\n",
    "  Args:\n",
    "    method: Name of modeling method (vanilla, dropout, svi, ll_svi).\n",
    "    architecture: Name of DNN architecture (mlp or dropout).\n",
    "    output_dir: Directory to record the trained model and output stats.\n",
    "    test_level: Zero indicates no testing. One indicates testing with real data.\n",
    "      Two is for testing with fake data.\n",
    "  \"\"\"\n",
    "  fake_data = test_level > 1\n",
    "  gfile.makedirs(output_dir)\n",
    "  model_opts, data_opts_list = get_experiment_config(method, architecture,\n",
    "                                                     test_level=test_level,\n",
    "                                                     output_dir=output_dir)\n",
    "\n",
    "  # Separately build dataset[0] with shuffle=True for training.\n",
    "  dataset_train = data_lib.build_dataset(data_opts_list[0], fake_data=fake_data)\n",
    "  dataset_eval = data_lib.build_dataset(data_opts_list[1], fake_data=fake_data)\n",
    "  model = models_lib.build_and_train(model_opts,\n",
    "                                     dataset_train, dataset_eval, output_dir)\n",
    "  logging.info('Saving model to output_dir.')\n",
    "  model.save_weights(output_dir + '/model.ckpt')\n",
    "\n",
    "  for idx, data_opts in enumerate(data_opts_list):\n",
    "    dataset = data_lib.build_dataset(data_opts, fake_data=fake_data)\n",
    "    logging.info('Running predictions for dataset #%d', idx)\n",
    "    stats = models_lib.make_predictions(model_opts, model, dataset)\n",
    "    array_utils.write_npz(output_dir, 'stats_%d.npz' % idx, stats)\n",
    "    del stats['logits_samples']\n",
    "    array_utils.write_npz(output_dir, 'stats_small_%d.npz' % idx, stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}